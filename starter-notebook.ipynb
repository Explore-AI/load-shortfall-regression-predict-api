{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7e849a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T09:24:53.643384Z",
     "start_time": "2021-06-11T09:24:53.622385Z"
    }
   },
   "source": [
    "# Regression Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "I {**YOUR NAME, YOUR SURNAME**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### Predict Overview: Spain Electricity Shortfall Challenge\n",
    "\n",
    "The government of Spain is considering an expansion of it's renewable energy resource infrastructure investments. As such, they require information on the trends and patterns of the countries renewable sources and fossil fuel energy generation. Your company has been awarded the contract to:\n",
    "\n",
    "- 1. analyse the supplied data;\n",
    "- 2. identify potential errors in the data and clean the existing data set;\n",
    "- 3. determine if additional features can be added to enrich the data set;\n",
    "- 4. build a model that is capable of forecasting the three hourly demand shortfalls;\n",
    "- 5. evaluate the accuracy of the best machine learning model;\n",
    "- 6. determine what features were most important in the model’s prediction decision, and\n",
    "- 7. explain the inner working of the model to a non-technical audience.\n",
    "\n",
    "Formally the problem statement was given to you, the senior data scientist, by your manager via email reads as follow:\n",
    "\n",
    "> In this project you are tasked to model the shortfall between the energy generated by means of fossil fuels and various renewable sources - for the country of Spain. The daily shortfall, which will be referred to as the target variable, will be modelled as a function of various city-specific weather features such as `pressure`, `wind speed`, `humidity`, etc. As with all data science projects, the provided features are rarely adequate predictors of the target variable. As such, you are required to perform feature engineering to ensure that you will be able to accurately model Spain's three hourly shortfalls.\n",
    " \n",
    "On top of this, she has provided you with a starter notebook containing vague explanations of what the main outcomes are. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05600c92",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997462e2",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475dbe93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T10:30:53.800892Z",
     "start_time": "2021-06-23T10:30:50.215449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "#Testing github\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Libraries for data preparation and model building\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Libraries for data preparation and model building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.graphics.correlation import plot_corr\n",
    "from sklearn import metrics #RMSE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Setting global constants to ensure notebook results are reproducible\n",
    "# PARAMETER_CONSTANT = ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a6718",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to load the data from the `df_train` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb6c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:35.311495Z",
     "start_time": "2021-06-28T08:49:35.295494Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_train.csv\"); # load the data\n",
    "df.head() #Testing github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1accf4e7",
   "metadata": {},
   "source": [
    "First things first, we had to load our train dataframe to be able to train our model(s), after loading we check a summary of what the dataframe consists of by using the .head() method and we can see after executing the cell that the dataframe has 49 columns.\n",
    "\n",
    "We will later use other methods to analyse our data further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"df_test.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9721a8a",
   "metadata": {},
   "source": [
    "Then we load our test dataframe which will help us in testing our model(s) and checked what data it consists of by using the .head() method also seeing that it consists of 48 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81132ab3",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a82a76",
   "metadata": {},
   "source": [
    "We begin by exploring the data by checking all columns for null values using a Pandas method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e805134e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:52:37.824204Z",
     "start_time": "2021-06-28T08:52:37.811206Z"
    }
   },
   "outputs": [],
   "source": [
    "# look at data statistics\n",
    "df.info();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e26f69",
   "metadata": {},
   "source": [
    "It is now clear to see that the column 'Valencia_pressure' has a significant number of null values, **2,068 null values** to be exact out of 8763 rows.\n",
    "\n",
    "In the cell below, the next step with regards to understanding the data is by using the 'describe' method to generate descriptive statistics which will provide insights regarding numerical columns.  This method shows us statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55bb337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b196a2b",
   "metadata": {},
   "source": [
    "The shape method will show us the amounts of rows the dataset contains, as well as the number of columns.\n",
    "\n",
    "After executing the cell we found that the train dataframe has 8763 rows and 49 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1111e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Number of rows and coloumns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc3964",
   "metadata": {},
   "source": [
    "The 'isnull()' method will be applied to confirm the number(sum) of null values in each column.  This analysis further confirms that the 'Valencia_pressure' column indeed contains 2068 null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Null Values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27de58f",
   "metadata": {},
   "source": [
    "Further inspection of the \"Valencia_pressure\" column is done to analyze the values it containts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae73977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valencia_pressure has 2068 NulLs\n",
    "df.Valencia_pressure.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d139b62a",
   "metadata": {},
   "source": [
    "Skew will indicate how symmetrical your data is.  \n",
    "\n",
    "The \"skew\" method used in the cell below indicates that the columns \"Bilbao_weather_id\", \"Madrid_pressure\", \"Valencia_pressure\", \"Barcelona_weather_id\", \"Madrid_weather_id\" have high negative skewness.  \n",
    "\n",
    "Amongst other columns containing high positive skewness, \"Bilbao_snow_3h\", \"Barcelona_pressure\", \"Seville_rain_3h\", \"Valencia_snow_3h\" were found to have dramatically high double-digit values for skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820be7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature distributions\n",
    "#Check for symmetrical data\\\\Most of our data is fairly symmetrical and o High positive skew\n",
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337a0d9",
   "metadata": {},
   "source": [
    "To measure the presence of outliers in the data, the uses \"kurtosis\" method.  A value below 3 indicates low kurtosis and thus indicates a lack of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13407dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for Outliers// indicates we have many outliers in our data\n",
    "df.kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c37a47e",
   "metadata": {},
   "source": [
    "After executing our kurtosis method, we can identify our highest outliers being \"Barcelona_rain_1h\", \"Seville_rain_1h\", \"Bilbao_snow_3h\", \"Barcelona_pressure\", \"Seville_rain_3h\", \"Madrid_rain_1h\", \"Baarcelona_rain_3h\" and \"Valencia_snow_3h\".\n",
    "\n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0028ee82",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7420a0c",
   "metadata": {},
   "source": [
    "A boxplot will now be used to display the distribution of data in terms of the “minimum”, first quartile, median, third quartile, and the maximum.  This is commonly referred to as the \"five number summary\". The boxplot shows one the outliers in the data and what the outlier values are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d12705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardisation or normalisation can be applied to a feature to adjust the range\n",
    "sns.boxplot(x='load_shortfall_3h', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x='load_shortfall_3h', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b1254",
   "metadata": {},
   "source": [
    "The visualisations below helps to graphically communicate the skewness that was measured above in this notebook when the \"skew()\" method was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38dee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Madrid_wind_speed', 'Madrid_humidity', 'Madrid_clouds_all', 'Madrid_rain_1h','Madrid_temp_max','Madrid_temp_min'] # create a list of all numerical features\n",
    "df[features].hist(figsize=(10,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2ee2d5",
   "metadata": {},
   "source": [
    "### Analyzing correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958719f",
   "metadata": {},
   "source": [
    "In the next step of our exploratory data analysis, we analyze correlation between columns. The corr method is used to find the pairwise correlation of all columns. \n",
    "\n",
    "All non-numeric data columns in the Dataframe are ignore in this step.\n",
    "\n",
    "Please take note that the correlation of a variable with itself is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate correlation//determine correlation between features\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec11fc",
   "metadata": {},
   "source": [
    "\"Madrid_temp_max\" and \"Valencia_temp_max\" show high correlation for example which may partly be explained by the two cities closeness in distance as compared to Barcelona for example which is around 300 kilometers further from Madrid than Valencia is.\n",
    "\n",
    "To see the level of correlation visually, a heatmap is used in the cell below to indicate correlation among variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate correlation\n",
    "plt.figure(figsize = (45, 25))\n",
    "heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, cmap=\"BuPu\", annot=True)\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at feature distributions\n",
    "df.hist(bins = 50, figsize = (40, 30), color = \"tab:blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa93ec6",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to: clean the dataset, and possibly create new features - as identified in the EDA phase. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23756218",
   "metadata": {},
   "source": [
    "Analyzing the data has assisted in identifying data errors. The next step is to update, change, or remove data to correct certain issues with the data. By doing this, data quality is improved. Improves data quality helps provide more accurate, consistent and reliable information to use in the predictive models.\n",
    "\n",
    "In the cell below, the 'Valencia_pressure' column's null values are updated and imputed with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing values/ features??//Valencia_pressure \n",
    "#Populate null values of Valencia_pressure with mean \n",
    "df['Valencia_pressure']=df['Valencia_pressure'].fillna(df['Valencia_pressure'].mean())\n",
    "\n",
    "df_test['Valencia_pressure'] = df_test['Valencia_pressure'].fillna(df_test['Valencia_pressure'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65deb614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Test \n",
    "df['Valencia_pressure']\n",
    "\n",
    "df_test['Valencia_pressure']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167f6260",
   "metadata": {},
   "source": [
    "The \"isnull\" method is now used on the \"Valencia_pressure\" column to confirm that the mean values have been substituted in place of the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ebfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Valencia_pressure.isnull().sum()\n",
    "\n",
    "df_test.Valencia_pressure.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd742e",
   "metadata": {},
   "source": [
    "Further analysis of the dataframe in the cell below shows the column that are \"object\" type.  This is an important step that may show the analysis team which columns may require their datatype to be changed in order o be useful for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eea17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new features\n",
    "#Check data which is in Object datatype\n",
    "df.select_dtypes(include=['object']).head(5)\n",
    "\n",
    "df_test.select_dtypes(include=['object']).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c0296",
   "metadata": {},
   "source": [
    "The \"time\" column contains valuable information that can be used later on.  Pandas is used in the cell below to change the datatype to numeric type within the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59692724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineer existing features\n",
    "#Change Objects to numeric,starting with time\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "df_test['time'] = pd.to_datetime(df_test['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f576b4",
   "metadata": {},
   "source": [
    "The \"Valencia_wind_deg\" column also contains valuable ordinal data (statistical data that is categorical where the variables have ordered categories).\n",
    "\n",
    "Python allows us to extract the relevant information (the digits, in this case) and by transforming the digits into a integer datatype, the data becomes useful for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Valencia_wind_deg'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e08c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove 'level_' on Valencia_wind_deg and convert to numeric datatype0\n",
    "df['Valencia_wind_deg'] =df['Valencia_wind_deg'].str.extract('(\\d+)')\n",
    "df['Valencia_wind_deg'] = pd.to_numeric(df['Valencia_wind_deg'])\n",
    "\n",
    "df_test['Valencia_wind_deg'] =df_test['Valencia_wind_deg'].str.extract('(\\d+)')\n",
    "df_test['Valencia_wind_deg'] = pd.to_numeric(df_test['Valencia_wind_deg'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d331d",
   "metadata": {},
   "source": [
    "Python also allows us to extract the relevant information from the \"Seville_pressure\" column and by transforming the digits into a integer datatype, the data becomes useful for further use, as can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing data for the Seville_pressure column which will be cleaned \n",
    "df['Seville_pressure'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dac11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove 'sp' on Seville_pressure data. and convert it numeric\n",
    "df['Seville_pressure'] = df['Seville_pressure'].str.extract('(\\d+)')\n",
    "df['Seville_pressure'] = pd.to_numeric(df['Seville_pressure'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Seville_pressure'] = df_test['Seville_pressure'].str.extract('(\\d+)')\n",
    "df_test['Seville_pressure'] = pd.to_numeric(df_test['Seville_pressure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda706a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete Markdown\n",
    "df = df.drop('Unnamed: 0',axis = 1)\n",
    "\n",
    "df_test = df_test.drop('Unnamed: 0',axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2d523",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to create one or more regression models that are able to accurately predict the thee hour load shortfall. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X = df.drop([\"load_shortfall_3h\", \"time\"], axis = 1) #split the feature variable\n",
    "y = df.iloc[:, -1] #split the response/target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data \n",
    "#y = df[:len(df_train)][[\"load_shortfall_3h\"]]\n",
    "#x = df[:len(df_train)].drop([\"load_shortfall_3h\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5112d9df",
   "metadata": {},
   "source": [
    "The above cell contains the features on which we will train our model(s) which is contained in the X variable and the response/target variable which we are trying to predict (load_shortfall_3h) in the y variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create targets and features dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b357a4",
   "metadata": {},
   "source": [
    "In our train-test function, we indicate the size of the test data we want to test with each iteration, which is typically a size of 0.2 (has to be between 0 and 1).\n",
    "\n",
    "And the random state ensures that the test picks data randomly when the train-test function is carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d073e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one or more ML models\n",
    "# Linear regression model\n",
    "\n",
    "lm = LinearRegression() #create the model\n",
    "lm.fit(X_train, y_train) #train the model\n",
    "predict = lm.predict(X_test) #predict on unseen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge model\n",
    "\n",
    "Ridge = Ridge() #create the model\n",
    "Ridge.fit(X_train, y_train) #train model\n",
    "R_pred = Ridge.predict(X_test) #predict on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a45b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest model\n",
    "R_F = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "R_F.fit(X_train,y_train)\n",
    "y_pred = R_F.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more ML models\n",
    "\n",
    "print('Linear model train:', np.sqrt(metrics.mean_squared_error(y_train, lm.predict(X_train))))\n",
    "print('Linear model test:', np.sqrt(metrics.mean_squared_error(y_test, predict)))\n",
    "\n",
    "print('Ridge model train:', np.sqrt(metrics.mean_squared_error(y_train, Ridge.predict(X_train))))\n",
    "print('Ridge model test:', np.sqrt(metrics.mean_squared_error(y_test, R_pred)))\n",
    "\n",
    "print('RandomForest model train:', np.sqrt(metrics.mean_squared_error(y_train, R_F.predict(X_train))))\n",
    "print('RandomForest model test:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2621fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more ML models\n",
    "#Evaluating using Decision Tree, to find a better explanation of data\n",
    "X = df[\"load_shortfall_3h\"] # independent variable\n",
    "y = df[\"Madrid_temp_max\"] # dependent variable\n",
    "\n",
    "plt.scatter(X,y) # create scatter plot\n",
    "plt.title(\"Madrid Max Temp vs Load Shrtfall in 3 hours\")\n",
    "plt.xlabel(\"Load_shortfall_3h \")\n",
    "plt.ylabel(\"Madrid_temp_max\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting our data to evaluate the performance of the model\n",
    "x_train, x_test, y_train, y_test = train_test_split(X[:,np.newaxis],y,test_size=0.2,random_state=42)\n",
    "# Instantiate regression tree model\n",
    "regr_tree = DecisionTreeRegressor(max_depth=2,random_state=42)\n",
    "regr_tree.fit(x_train,y_train)\n",
    "DecisionTreeRegressor(max_depth=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the Decision tree\n",
    "plt.figure(figsize=(9,9))\n",
    "_ = plot_tree(regr_tree, feature_names=['Max Temp vs Load Shotfall in 3h'],  filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aef953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Model Performance\n",
    "# get predictions for test data\n",
    "y_pred = regr_tree.predict(x_test)\n",
    "\n",
    "# calculate MSE\n",
    "MSE = mean_squared_error(y_pred,y_test)\n",
    "\n",
    "# Report RMSE\n",
    "print(\"Regression Decision Tree model RMSE is:\",np.sqrt(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7920def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising Model Output\n",
    "x_domain = np.linspace(min(X), max(X), 100)[:, np.newaxis]\n",
    "# predict y for every point in x-domain\n",
    "y_predictions = regr_tree.predict(x_domain)\n",
    "# plot the regression tree line over data\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.plot(x_domain, y_predictions, color=\"red\", label='predictions')\n",
    "plt.xlabel(\"Madrid Max Temp vs Load Shrtfall in 3 hours\")\n",
    "plt.ylabel(\"Madrid Max Temp \")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d2b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MRL Model\n",
    "# split predictors and response\n",
    "X = df.drop([\"load_shortfall_3h\", \"time\"], axis = 1) #split the feature variable\n",
    "y = df.iloc[:, -1] #split the response/target variable\n",
    "\n",
    "lm = LinearRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d37f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X_train, y_train)\n",
    "# extract model intercept\n",
    "beta_0 = float(lm.intercept_)\n",
    "# extract model coeffs\n",
    "beta_js = pd.DataFrame(lm.coef_, X.columns, columns=['Coefficient'])\n",
    "print(\"Intercept is\", beta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d83a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show Model\n",
    "fig, axs = plt.subplots(2, 2, figsize=(9,7))\n",
    "\n",
    "axs[0,0].scatter(df['Bilbao_snow_3h'], df[\"load_shortfall_3h\"])\n",
    "axs[0,0].plot(df['Bilbao_snow_3h'], lm.intercept_ + lm.coef_[18]*df['Bilbao_snow_3h'], color='red')\n",
    "axs[0,0].title.set_text('Bilbao_snow_3h vs. load_shortfall_3h')\n",
    "\n",
    "axs[0,1].scatter(df['Seville_rain_3h'], df[\"load_shortfall_3h\"])\n",
    "axs[0,1].plot(df['Seville_rain_3h'], lm.intercept_ + lm.coef_[20]*df['Seville_rain_3h'], color='green')\n",
    "axs[0,1].title.set_text('Seville_rain_3h vs. load_shortfall_3h')\n",
    "\n",
    "axs[1,0].scatter(df['Madrid_temp_max'], df[\"load_shortfall_3h\"])\n",
    "axs[1,0].plot(df['Madrid_temp_max'], lm.intercept_ + lm.coef_[38]*df['Madrid_temp_max'], color='blue')\n",
    "axs[1,0].title.set_text('Madrid_temp_max vs. load_shortfall_3h')\n",
    "\n",
    "axs[1,1].scatter(df['Madrid_wind_speed'], df[\"load_shortfall_3h\"])\n",
    "axs[1,1].plot(df['Madrid_wind_speed'], lm.intercept_ + lm.coef_[1]*df['Madrid_wind_speed'], color='blue')\n",
    "axs[1,1].title.set_text('Madrid_wind_speed vs. load_shortfall_3h')\n",
    "\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b530251",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more ML models\n",
    "\n",
    "print('Linear model train:', np.sqrt(metrics.mean_squared_error(y_train, lm.predict(X_train))))\n",
    "print('Linear model test:', np.sqrt(metrics.mean_squared_error(y_test, predict)))\n",
    "\n",
    "print('Ridge model train:', np.sqrt(metrics.mean_squared_error(y_train, Ridge.predict(X_train))))\n",
    "print('Ridge model test:', np.sqrt(metrics.mean_squared_error(y_test, R_pred)))\n",
    "\n",
    "print('RandomForest model train:', np.sqrt(metrics.mean_squared_error(y_train, R_F.predict(X_train))))\n",
    "print('RandomForest model test:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best model and motivate why it is the best choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad0c0d",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff741c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss chosen methods logic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "0091ca96dceaf78aa4899f24ebdea97c56a9ee4e178f32e70c35df1c77bdf4b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
